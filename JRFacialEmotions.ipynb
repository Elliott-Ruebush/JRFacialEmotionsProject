{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get all our imports done with\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as mplot\n",
    "import os\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory is: E:\\Users\\Elliott\\Documents\\CKData\\Emotion\n",
      "Disgust\n",
      "\n",
      "\n",
      "\n",
      "Printing dictionary of image file/emotion pairs:\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "#This whole block of code is sloppy and inefficient and has code that defeats the point of using other code, but it gets the job done for now. TODO: Format, cleanup unneeded bits of code\n",
    "\n",
    "#Organizes the files into lists and a dict we can use for our algorithms and defines a function to \n",
    "\n",
    "#Change directory to the directory our data is stored in\n",
    "os.chdir(\"E:\\Users\\Elliott\\Documents\\CKData\\Emotion\")\n",
    "print(\"Current directory is: \" + os.getcwd())\n",
    "\n",
    "#Import emotion labels \n",
    "emotionIDList = [\"Neutral\", \"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "#Initialize dictionary and lists for storing emotion data and image file locations \n",
    "ImageIDEmotionPair = {}\n",
    "peakEmotionFileLocations = []\n",
    "neutralEmotionFileLocations = []\n",
    "\n",
    "emotionsRootDir = \"E:\\Users\\Elliott\\Documents\\CKData\\Emotion\"\n",
    "imagesRootDir = \"E:\\Users\\Elliott\\Documents\\CKData\\extended-cohn-kanade-images\\cohn-kanade-images\"\n",
    "for dirName, subDirList, fileList in os.walk(emotionsRootDir):\n",
    "    if len(fileList) > 0:\n",
    "        for fileName in fileList:\n",
    "            #Change the directory to the directory containing the file we want to access\n",
    "            lowestDir = os.path.normpath(fileName[:9].replace(\"_\", \"/\"))\n",
    "            os.chdir('E:\\Users\\Elliott\\Documents\\CKData\\Emotion\\%s' % (lowestDir))\n",
    "            #Read the text from the .txt file that denotes the emotion of the penultimate image of a sequence\n",
    "            fileEmotionTxt = open(fileName, \"r\")\n",
    "            emotionNumID = int(fileEmotionTxt.read()[3:4])\n",
    "            fileEmotionTxt.close()\n",
    "            #Put the Photo IDs and the corresponding emotions(Number converted to a string of the emotion) together \n",
    "            #into a dict as a key:value pair\n",
    "            slicedFileName = fileName[:17]\n",
    "            ImageIDEmotionPair[slicedFileName] = emotionIDList[emotionNumID]\n",
    "            neutralImage = imagesRootDir + os.path.normpath(\"/\" + lowestDir + \"/\" + fileName[:15] + \"00\")\n",
    "            #print(neutralImage)\n",
    "            #Create a list of all the files with peak emotion and a list of all the neutral files (Could do this better in another os.walk for loop)\n",
    "            peakEmotionFileLocations.append(imagesRootDir + os.path.normpath(\"/\" + lowestDir + \"/\" + fileName[:17]))\n",
    "            \n",
    "#Iterating through the list directly with \"for x in list\" doesn't work here, hence the different for loop\n",
    "for x in range(0, len(neutralEmotionFileLocations)):\n",
    "    neutralEmotionFileLocations[x] = neutralEmotionFileLocations[x] + \"00\"\n",
    "    \n",
    "def getFileEmotion(fileLocation):\n",
    "    fileLocation = fileLocation.replace(imagesRootDir, \"\")\n",
    "    editedFileLocation = fileLocation[10:]\n",
    "    return ImageIDEmotionPair[editedFileLocation]\n",
    "\n",
    "print(getFileEmotion(peakEmotionFileLocations[0]))\n",
    "print(\"\\n\\n\\nPrinting dictionary of image file/emotion pairs:\\n%s\" % len(sorted(ImageIDEmotionPair.values())))\n",
    "#print(\"\\n\" + getFileEmotion(peakEmotionFileLocations[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the images to grayscale and isolate the faces --- Only needed to run once!!!\n",
    "\n",
    "# #based off of http://www.paulvangent.com/2016/04/01/emotion-recognition-with-python-opencv-and-a-face-dataset/\n",
    "# faceDet = cv2.CascadeClassifier(\"E:\\Users\\Elliott\\opencv\\sources\\data\\haarcascades\\haarcascade_frontalface_default.xml\")\n",
    "# faceDet_two = cv2.CascadeClassifier(\"E:\\Users\\Elliott\\opencv\\sources\\data\\haarcascades\\haarcascade_frontalface_alt2.xml\")\n",
    "# faceDet_three = cv2.CascadeClassifier(\"E:\\Users\\Elliott\\opencv\\sources\\data\\haarcascades\\haarcascade_frontalface_alt.xml\")\n",
    "# faceDet_four = cv2.CascadeClassifier(\"E:\\Users\\Elliott\\opencv\\sources\\data\\haarcascades\\haarcascade_frontalface_alt_tree.xml\")\n",
    "\n",
    "\n",
    "# def detectFaces(emotion):\n",
    "#     filenumber = 0\n",
    "#     currentEmotionFilesList = []\n",
    "#     for fileName in peakEmotionFileLocations:\n",
    "#         if getFileEmotion(fileName) == emotion:\n",
    "#             currentEmotionFilesList.append(fileName)\n",
    "#     for aFile in currentEmotionFilesList:\n",
    "#         aFile = aFile + \".png\"\n",
    "#         #Image reading and conversion to grayscale \n",
    "#         frame = cv2.imread(aFile) #Open image\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "        \n",
    "#         #Detect face using 4 different classifiers\n",
    "#         face = faceDet.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "#         face_two = faceDet_two.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "#         face_three = faceDet_three.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "#         face_four = faceDet_four.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "        \n",
    "#     #Go over detected faces, stop at first detected face, return empty if no face.\n",
    "#         if len(face) == 1:\n",
    "#             facefeatures = face\n",
    "#         elif len(face_two) == 1:\n",
    "#             facefeatures = face_two\n",
    "#         elif len(face_three) == 1:\n",
    "#             facefeatures = face_three\n",
    "#         elif len(face_four) == 1:\n",
    "#             facefeatures = face_four\n",
    "#         else:\n",
    "#             facefeatures = \"\"\n",
    "    \n",
    "#     #Cut and save face\n",
    "#         for (x, y, w, h) in facefeatures: #get coordinates and size of rectangle containing face\n",
    "#             print \"Face found in file: %s\" % aFile\n",
    "#             gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "        \n",
    "#             try:\n",
    "#                 out = cv2.resize(gray, (350, 350)) #Resize face so all images have same size\n",
    "#                 print(\"Writing file...\")\n",
    "#                 cv2.imwrite(\"E:\\\\Users\\\\Elliott\\\\Documents\\\\CKData\\\\EditedGrayScaleFaces\\\\%s\\\\%s.jpg\" %(emotion, filenumber), out) #Write image\n",
    "#             except:\n",
    "#                 pass #If error, pass file\n",
    "#         filenumber += 1 #Increment image number\n",
    "        \n",
    "# #Edited to avoid neutral until made compatible\n",
    "# for emotion in emotionIDList[1:8]:\n",
    "#     os.makedirs(\"E:\\\\Users\\\\Elliott\\\\Documents\\\\CKData\\\\EditedGrayScaleFaces\\\\%s\" % emotion)\n",
    "#     print(emotion)\n",
    "#     detectFaces(emotion) #Call function\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Organize the data into training and prediction sets\n",
    "#THIS PIECE OF CODE CAN PROBABLY ALSO BE IMPLEMENTED MORE EFFICIENTLY\n",
    "from random import shuffle\n",
    "def splitEmotionData(emotion):\n",
    "    currentDir = \"E:\\\\Users\\\\Elliott\\\\Documents\\\\CKData\\\\EditedGrayScaleFaces\\\\%s\" % emotion\n",
    "    currentEmotionFilesList = []\n",
    "    for aFile in os.listdir(currentDir):\n",
    "        currentEmotionFilesList.append(currentDir + os.path.normpath(\"/\" + aFile))\n",
    "    shuffle(currentEmotionFilesList)\n",
    "    trainingSet = currentEmotionFilesList[:int(len(currentEmotionFilesList) * .8)]\n",
    "    testingSet = currentEmotionFilesList[-int(len(currentEmotionFilesList) * .2):]\n",
    "#     print(len(trainingSet))\n",
    "#     print(len(testingSet))\n",
    "    #Length is one off but we worry about that later\n",
    "#     print(len(currentEmotionFilesList))\n",
    "    \n",
    "    return trainingSet, testingSet\n",
    "    \n",
    "def createSets():\n",
    "    trainingSetFiles = []\n",
    "    trainingEmotions = []\n",
    "    testingSetFiles = []\n",
    "    testingEmotions = []\n",
    "    for emotion in emotionIDList[1:8]:\n",
    "        training, testing = splitEmotionData(emotion)\n",
    "        #Create emotion labels for each set\n",
    "        for item in training:\n",
    "            item = item\n",
    "            image = cv2.imread(item) #open image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "            trainingSetFiles.append(gray) #append image array to training data list\n",
    "            trainingEmotions.append(emotionIDList.index(emotion))\n",
    "        for item in testing:\n",
    "            item = item\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            testingSetFiles.append(gray)\n",
    "            testingEmotions.append(emotionIDList.index(emotion))\n",
    "            \n",
    "    return trainingSetFiles, trainingEmotions, testingSetFiles, testingEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training fisher face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 66 percent correct!\n",
      "45\n",
      "Amount correct for emotion(Anger): 5\n",
      "Percent of this emotion(Anger) gotten correct: 55.00\n",
      "34 percent incorrect!\n",
      "Amount incorrect for emotion(Anger): 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float argument required, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-14784b2876f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Amount incorrect for emotion(%s): %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0memotionIDList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamountIncorrectPerEmotion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0memotionIDList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mpercentIncorrectForEmotion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Percent of this emotion(%s) gotten incorrect: %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0memotionIDList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercentIncorrectForEmotion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mmetascore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float argument required, not tuple"
     ]
    }
   ],
   "source": [
    "#run tests\n",
    "fisherFaces = cv2.createFisherFaceRecognizer()\n",
    "\n",
    "#http://www.paulvangent.com/2016/04/01/emotion-recognition-with-python-opencv-and-a-face-dataset/\n",
    "def runFisherFaces():\n",
    "    trainingSetFiles, trainingEmotions, testingSetFiles, testingEmotions = createSets()\n",
    "    incorrectEmotions = {}\n",
    "    correctEmotions = {}\n",
    "    for emotion in emotionIDList[1:8]:\n",
    "        incorrectEmotions[emotion] = 0\n",
    "        correctEmotions[emotion] = 0\n",
    "    print \"\\n\\n\\nTraining fisher face classifier...\"\n",
    "    print \"Size of training set is:\", len(trainingEmotions), \"images\"\n",
    "    fisherFaces.train(trainingSetFiles, np.asarray(trainingEmotions))\n",
    "\n",
    "    print \"Predicting classification set...\"\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for image in testingSetFiles:\n",
    "        pred, conf = fisherFaces.predict(image)\n",
    "        if pred == testingEmotions[count]:\n",
    "            correctEmotions[emotionIDList[testingEmotions[count]]] += 1\n",
    "            correct += 1\n",
    "            count += 1\n",
    "        else:\n",
    "            incorrectEmotions[emotionIDList[testingEmotions[count]]] += 1\n",
    "            incorrect += 1\n",
    "            count += 1\n",
    "    return ((100*correct)/(correct + incorrect)), incorrectEmotions, correctEmotions\n",
    "\n",
    "#Now run it\n",
    "metascore = []\n",
    "# confMatrix = {}\n",
    "# for emotion in emotionIDList[1:7]:\n",
    "#     tempList = []\n",
    "#     confMatrix[emotion] = tempList\n",
    "for i in range(0,1):\n",
    "    correct, amountIncorrectPerEmotion, amountCorrectPerEmotion = runFisherFaces()\n",
    "    print (\"Got %s percent correct!\" % str(correct))\n",
    "    for x in range(1, len(emotionIDList[1:8]) + 1):\n",
    "        amountCorrect = amountCorrectPerEmotion[emotionIDList[x]]\n",
    "        amountIncorrect = amountIncorrectPerEmotion[emotionIDList[x]]\n",
    "        \n",
    "        percentCorrectForEmotion = (100 * amountCorrect) / (amountCorrect + amountIncorrect)\n",
    "        percentIncorrectForEmotion = (100 - percentCorrectForEmotion)\n",
    "        print percentIncorrectForEmotion\n",
    "        print(\"Amount correct for emotion(%s): %d\" % (emotionIDList[x], amountCorrect))\n",
    "        \n",
    "        print(\"Percent of this emotion(%s) gotten correct: %.2f\" % (emotionIDList[x], percentCorrectForEmotion))\n",
    "    \n",
    "        print 100 - correct, \"percent incorrect!\"\n",
    "        print(\"Amount incorrect for emotion(%s): %d\" % (emotionIDList[x], amountIncorrectPerEmotion[emotionIDList[x]]))\n",
    "        percentIncorrectForEmotion = ()\n",
    "        print(\"Percent of this emotion(%s) gotten incorrect: %.2f\" % (emotionIDList[x], percentIncorrectForEmotion))\n",
    "    \n",
    "    metascore.append(correct)\n",
    "\n",
    "print \"\\nEnd score:\", np.mean(metascore), \"percent correct!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 22 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 16 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 17 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 16 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 27 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 29 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 17 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 25 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 16 percent correct!\n",
      "Training Eigen face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 14 percent correct!\n",
      "\n",
      "End score: 19.9 percent correct!\n"
     ]
    }
   ],
   "source": [
    "eigenFaces = cv2.createEigenFaceRecognizer()\n",
    "\n",
    "def runEigenFaces():\n",
    "    trainingSetFiles, trainingEmotions, testingSetFiles, testingEmotions = createSets()\n",
    "    \n",
    "    print \"Training Eigen face classifier...\"\n",
    "    print \"Size of training set is:\", len(trainingEmotions), \"images\"\n",
    "    eigenFaces.train(trainingSetFiles, np.asarray(trainingEmotions))\n",
    "\n",
    "    print \"Predicting classification set...\"\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for image in testingSetFiles:\n",
    "        pred, conf = eigenFaces.predict(image)\n",
    "        if pred == testingEmotions[count]:\n",
    "            correct += 1\n",
    "            count += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            count += 1\n",
    "    return ((100*correct)/(correct + incorrect))\n",
    "\n",
    "#Now run it\n",
    "metascore = []\n",
    "for i in range(0,10):\n",
    "    correct = runEigenFaces()\n",
    "    print (\"Got %s percent correct!\" % str(correct))\n",
    "    metascore.append(correct)\n",
    "\n",
    "print \"\\nEnd score:\", np.mean(metascore), \"percent correct!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LBPH face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 22 percent correct!\n",
      "Training LBPH face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 30 percent correct!\n",
      "Training LBPH face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 24 percent correct!\n",
      "Training LBPH face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 20 percent correct!\n",
      "Training LBPH face classifier...\n",
      "Size of training set is: 260 images\n",
      "Predicting classification set...\n",
      "Got 19 percent correct!\n",
      "\n",
      "End score: 23.0 percent correct!\n"
     ]
    }
   ],
   "source": [
    "LBPHFaces = cv2.createLBPHFaceRecognizer()\n",
    "\n",
    "def runLBPHFaces():\n",
    "    trainingSetFiles, trainingEmotions, testingSetFiles, testingEmotions = createSets()\n",
    "    \n",
    "    print \"Training LBPH face classifier...\"\n",
    "    print \"Size of training set is:\", len(trainingEmotions), \"images\"\n",
    "    LBPHFaces.train(trainingSetFiles, np.asarray(trainingEmotions))\n",
    "\n",
    "    print \"Predicting classification set...\"\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for image in testingSetFiles:\n",
    "        pred, conf = LBPHFaces.predict(image)\n",
    "        if pred == testingEmotions[count]:\n",
    "            correct += 1\n",
    "            count += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            count += 1\n",
    "    return ((100*correct)/(correct + incorrect))\n",
    "\n",
    "#Now run it\n",
    "metascore = []\n",
    "for i in range(0,5):\n",
    "    correct = runLBPHFaces()\n",
    "    print (\"Got %s percent correct!\" % str(correct))\n",
    "    metascore.append(correct)\n",
    "\n",
    "print \"\\nEnd score:\", np.mean(metascore), \"percent correct!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
